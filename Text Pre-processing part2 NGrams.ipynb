{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\"\n",
    "s = s.lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['le',\n",
       " 'temps',\n",
       " 'est',\n",
       " 'un',\n",
       " 'grand',\n",
       " 'maître',\n",
       " 'dit',\n",
       " 'on',\n",
       " 'le',\n",
       " 'malheur',\n",
       " 'est',\n",
       " \"qu'il\",\n",
       " 'tue',\n",
       " 'ses',\n",
       " 'élèves']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
    "s_tokenized = tokenizer.tokenize(s)\n",
    "s_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('_', '_', '_', 'l'),\n",
       "  ('_', '_', 'l', 'e'),\n",
       "  ('_', 'l', 'e', '_'),\n",
       "  ('l', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'e'),\n",
       "  ('_', 't', 'e', 'm'),\n",
       "  ('t', 'e', 'm', 'p'),\n",
       "  ('e', 'm', 'p', 's'),\n",
       "  ('m', 'p', 's', '_'),\n",
       "  ('p', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'e'),\n",
       "  ('_', '_', 'e', 's'),\n",
       "  ('_', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '_'),\n",
       "  ('s', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'u'),\n",
       "  ('_', '_', 'u', 'n'),\n",
       "  ('_', 'u', 'n', '_'),\n",
       "  ('u', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'g'),\n",
       "  ('_', '_', 'g', 'r'),\n",
       "  ('_', 'g', 'r', 'a'),\n",
       "  ('g', 'r', 'a', 'n'),\n",
       "  ('r', 'a', 'n', 'd'),\n",
       "  ('a', 'n', 'd', '_'),\n",
       "  ('n', 'd', '_', '_'),\n",
       "  ('d', '_', '_', '_')],\n",
       " [('_', '_', '_', 'm'),\n",
       "  ('_', '_', 'm', 'a'),\n",
       "  ('_', 'm', 'a', 'î'),\n",
       "  ('m', 'a', 'î', 't'),\n",
       "  ('a', 'î', 't', 'r'),\n",
       "  ('î', 't', 'r', 'e'),\n",
       "  ('t', 'r', 'e', '_'),\n",
       "  ('r', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'd'),\n",
       "  ('_', '_', 'd', 'i'),\n",
       "  ('_', 'd', 'i', 't'),\n",
       "  ('d', 'i', 't', '_'),\n",
       "  ('i', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'o'),\n",
       "  ('_', '_', 'o', 'n'),\n",
       "  ('_', 'o', 'n', '_'),\n",
       "  ('o', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'l'),\n",
       "  ('_', '_', 'l', 'e'),\n",
       "  ('_', 'l', 'e', '_'),\n",
       "  ('l', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'm'),\n",
       "  ('_', '_', 'm', 'a'),\n",
       "  ('_', 'm', 'a', 'l'),\n",
       "  ('m', 'a', 'l', 'h'),\n",
       "  ('a', 'l', 'h', 'e'),\n",
       "  ('l', 'h', 'e', 'u'),\n",
       "  ('h', 'e', 'u', 'r'),\n",
       "  ('e', 'u', 'r', '_'),\n",
       "  ('u', 'r', '_', '_'),\n",
       "  ('r', '_', '_', '_')],\n",
       " [('_', '_', '_', 'e'),\n",
       "  ('_', '_', 'e', 's'),\n",
       "  ('_', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '_'),\n",
       "  ('s', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'q'),\n",
       "  ('_', '_', 'q', 'u'),\n",
       "  ('_', 'q', 'u', \"'\"),\n",
       "  ('q', 'u', \"'\", 'i'),\n",
       "  ('u', \"'\", 'i', 'l'),\n",
       "  (\"'\", 'i', 'l', '_'),\n",
       "  ('i', 'l', '_', '_'),\n",
       "  ('l', '_', '_', '_')],\n",
       " [('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'u'),\n",
       "  ('_', 't', 'u', 'e'),\n",
       "  ('t', 'u', 'e', '_'),\n",
       "  ('u', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 's'),\n",
       "  ('_', '_', 's', 'e'),\n",
       "  ('_', 's', 'e', 's'),\n",
       "  ('s', 'e', 's', '_'),\n",
       "  ('e', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'é'),\n",
       "  ('_', '_', 'é', 'l'),\n",
       "  ('_', 'é', 'l', 'è'),\n",
       "  ('é', 'l', 'è', 'v'),\n",
       "  ('l', 'è', 'v', 'e'),\n",
       "  ('è', 'v', 'e', 's'),\n",
       "  ('v', 'e', 's', '_'),\n",
       "  ('e', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.util import ngrams\n",
    "generated_4grams = []\n",
    "\n",
    "for word in s_tokenized:\n",
    "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))) # n = 4.\n",
    "generated_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_3grams=[]\n",
    "for word in s_tokenized:\n",
    "    generated_3grams.append(list(ngrams(word, 3, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))) # n = 3.\n",
    "#generated_3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', '_', '_', 'l'),\n",
       " ('_', '_', 'l', 'e'),\n",
       " ('_', 'l', 'e', '_'),\n",
       " ('l', 'e', '_', '_'),\n",
       " ('e', '_', '_', '_'),\n",
       " ('_', '_', '_', 't'),\n",
       " ('_', '_', 't', 'e'),\n",
       " ('_', 't', 'e', 'm'),\n",
       " ('t', 'e', 'm', 'p'),\n",
       " ('e', 'm', 'p', 's')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
    "generated_4grams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Obtaining n-grams (n = 4)¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['___l',\n",
       " '__le',\n",
       " '_le_',\n",
       " 'le__',\n",
       " 'e___',\n",
       " '___t',\n",
       " '__te',\n",
       " '_tem',\n",
       " 'temp',\n",
       " 'emps',\n",
       " 'mps_',\n",
       " 'ps__',\n",
       " 's___',\n",
       " '___e',\n",
       " '__es',\n",
       " '_est',\n",
       " 'est_',\n",
       " 'st__',\n",
       " 't___',\n",
       " '___u',\n",
       " '__un',\n",
       " '_un_',\n",
       " 'un__',\n",
       " 'n___',\n",
       " '___g',\n",
       " '__gr',\n",
       " '_gra',\n",
       " 'gran',\n",
       " 'rand',\n",
       " 'and_',\n",
       " 'nd__',\n",
       " 'd___',\n",
       " '___m',\n",
       " '__ma',\n",
       " '_maî',\n",
       " 'maît',\n",
       " 'aîtr',\n",
       " 'ître',\n",
       " 'tre_',\n",
       " 're__',\n",
       " 'e___',\n",
       " '___d',\n",
       " '__di',\n",
       " '_dit',\n",
       " 'dit_',\n",
       " 'it__',\n",
       " 't___',\n",
       " '___o',\n",
       " '__on',\n",
       " '_on_',\n",
       " 'on__',\n",
       " 'n___',\n",
       " '___l',\n",
       " '__le',\n",
       " '_le_',\n",
       " 'le__',\n",
       " 'e___',\n",
       " '___m',\n",
       " '__ma',\n",
       " '_mal',\n",
       " 'malh',\n",
       " 'alhe',\n",
       " 'lheu',\n",
       " 'heur',\n",
       " 'eur_',\n",
       " 'ur__',\n",
       " 'r___',\n",
       " '___e',\n",
       " '__es',\n",
       " '_est',\n",
       " 'est_',\n",
       " 'st__',\n",
       " 't___',\n",
       " '___q',\n",
       " '__qu',\n",
       " \"_qu'\",\n",
       " \"qu'i\",\n",
       " \"u'il\",\n",
       " \"'il_\",\n",
       " 'il__',\n",
       " 'l___',\n",
       " '___t',\n",
       " '__tu',\n",
       " '_tue',\n",
       " 'tue_',\n",
       " 'ue__',\n",
       " 'e___',\n",
       " '___s',\n",
       " '__se',\n",
       " '_ses',\n",
       " 'ses_',\n",
       " 'es__',\n",
       " 's___',\n",
       " '___é',\n",
       " '__él',\n",
       " '_élè',\n",
       " 'élèv',\n",
       " 'lève',\n",
       " 'èves',\n",
       " 'ves_',\n",
       " 'es__',\n",
       " 's___',\n",
       " '__l',\n",
       " '_le',\n",
       " 'le_',\n",
       " 'e__',\n",
       " '__t',\n",
       " '_te',\n",
       " 'tem',\n",
       " 'emp',\n",
       " 'mps',\n",
       " 'ps_',\n",
       " 's__',\n",
       " '__e',\n",
       " '_es',\n",
       " 'est',\n",
       " 'st_',\n",
       " 't__',\n",
       " '__u',\n",
       " '_un',\n",
       " 'un_',\n",
       " 'n__',\n",
       " '__g',\n",
       " '_gr',\n",
       " 'gra',\n",
       " 'ran',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__m',\n",
       " '_ma',\n",
       " 'maî',\n",
       " 'aît',\n",
       " 'îtr',\n",
       " 'tre',\n",
       " 're_',\n",
       " 'e__',\n",
       " '__d',\n",
       " '_di',\n",
       " 'dit',\n",
       " 'it_',\n",
       " 't__',\n",
       " '__o',\n",
       " '_on',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__l',\n",
       " '_le',\n",
       " 'le_',\n",
       " 'e__',\n",
       " '__m',\n",
       " '_ma',\n",
       " 'mal',\n",
       " 'alh',\n",
       " 'lhe',\n",
       " 'heu',\n",
       " 'eur',\n",
       " 'ur_',\n",
       " 'r__',\n",
       " '__e',\n",
       " '_es',\n",
       " 'est',\n",
       " 'st_',\n",
       " 't__',\n",
       " '__q',\n",
       " '_qu',\n",
       " \"qu'\",\n",
       " \"u'i\",\n",
       " \"'il\",\n",
       " 'il_',\n",
       " 'l__',\n",
       " '__t',\n",
       " '_tu',\n",
       " 'tue',\n",
       " 'ue_',\n",
       " 'e__',\n",
       " '__s',\n",
       " '_se',\n",
       " 'ses',\n",
       " 'es_',\n",
       " 's__',\n",
       " '__é',\n",
       " '_él',\n",
       " 'élè',\n",
       " 'lèv',\n",
       " 'ève',\n",
       " 'ves',\n",
       " 'es_',\n",
       " 's__']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_list_4grams = generated_4grams\n",
    "for idx, val in enumerate(generated_4grams):\n",
    "    ng_list_4grams[idx] = ''.join(val)\n",
    "ng_list_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e___', 4),\n",
       " ('e__', 4),\n",
       " ('t__', 3),\n",
       " ('s___', 3),\n",
       " ('s__', 3),\n",
       " ('t___', 3),\n",
       " ('___e', 2),\n",
       " ('n___', 2),\n",
       " ('__m', 2),\n",
       " ('st__', 2),\n",
       " ('n__', 2),\n",
       " ('es_', 2),\n",
       " ('_es', 2),\n",
       " ('_le', 2),\n",
       " ('_est', 2),\n",
       " ('___t', 2),\n",
       " ('__t', 2),\n",
       " ('__es', 2),\n",
       " ('le_', 2),\n",
       " ('est_', 2),\n",
       " ('_le_', 2),\n",
       " ('__ma', 2),\n",
       " ('est', 2),\n",
       " ('es__', 2),\n",
       " ('_ma', 2),\n",
       " ('__le', 2),\n",
       " ('st_', 2),\n",
       " ('__e', 2),\n",
       " ('___l', 2),\n",
       " ('___m', 2),\n",
       " ('__l', 2),\n",
       " ('le__', 2),\n",
       " ('emp', 1),\n",
       " ('_qu', 1),\n",
       " ('__é', 1),\n",
       " ('mps', 1),\n",
       " ('ue_', 1),\n",
       " ('d___', 1),\n",
       " ('__gr', 1),\n",
       " ('ses', 1),\n",
       " ('__o', 1),\n",
       " ('heur', 1),\n",
       " ('l___', 1),\n",
       " ('ur_', 1),\n",
       " ('tue_', 1),\n",
       " ('_on_', 1),\n",
       " ('_él', 1),\n",
       " ('aît', 1),\n",
       " ('___d', 1),\n",
       " ('___u', 1),\n",
       " ('_ses', 1),\n",
       " ('_un', 1),\n",
       " ('__tu', 1),\n",
       " ('_tu', 1),\n",
       " (\"u'i\", 1),\n",
       " ('__qu', 1),\n",
       " ('dit_', 1),\n",
       " ('___g', 1),\n",
       " ('___q', 1),\n",
       " ('ps__', 1),\n",
       " ('re_', 1),\n",
       " (\"u'il\", 1),\n",
       " (\"qu'i\", 1),\n",
       " ('aîtr', 1),\n",
       " ('gran', 1),\n",
       " ('___é', 1),\n",
       " ('nd__', 1),\n",
       " ('lhe', 1),\n",
       " ('èves', 1),\n",
       " ('gra', 1),\n",
       " ('_di', 1),\n",
       " ('l__', 1),\n",
       " ('d__', 1),\n",
       " ('il_', 1),\n",
       " ('tre_', 1),\n",
       " ('on_', 1),\n",
       " ('___o', 1),\n",
       " ('rand', 1),\n",
       " ('malh', 1),\n",
       " ('__on', 1),\n",
       " ('emps', 1),\n",
       " ('mal', 1),\n",
       " ('un_', 1),\n",
       " ('it__', 1),\n",
       " ('ves_', 1),\n",
       " ('_tem', 1),\n",
       " ('and_', 1),\n",
       " ('_tue', 1),\n",
       " (\"'il\", 1),\n",
       " ('alhe', 1),\n",
       " ('ur__', 1),\n",
       " ('tre', 1),\n",
       " ('_maî', 1),\n",
       " ('_on', 1),\n",
       " ('îtr', 1),\n",
       " ('___s', 1),\n",
       " ('ève', 1),\n",
       " ('_un_', 1),\n",
       " ('_te', 1),\n",
       " ('on__', 1),\n",
       " ('eur', 1),\n",
       " ('alh', 1),\n",
       " ('ps_', 1),\n",
       " ('__g', 1),\n",
       " ('__q', 1),\n",
       " ('mps_', 1),\n",
       " ('ses_', 1),\n",
       " ('tue', 1),\n",
       " ('heu', 1),\n",
       " ('maît', 1),\n",
       " ('_mal', 1),\n",
       " ('__se', 1),\n",
       " ('__u', 1),\n",
       " ('ître', 1),\n",
       " ('ran', 1),\n",
       " ('maî', 1),\n",
       " ('élèv', 1),\n",
       " ('_dit', 1),\n",
       " ('r__', 1),\n",
       " ('_gr', 1),\n",
       " ('ves', 1),\n",
       " ('_gra', 1),\n",
       " ('nd_', 1),\n",
       " ('and', 1),\n",
       " ('re__', 1),\n",
       " (\"'il_\", 1),\n",
       " ('__un', 1),\n",
       " ('__te', 1),\n",
       " ('__él', 1),\n",
       " ('lèv', 1),\n",
       " ('__d', 1),\n",
       " ('_se', 1),\n",
       " ('ue__', 1),\n",
       " ('dit', 1),\n",
       " ('il__', 1),\n",
       " ('it_', 1),\n",
       " ('tem', 1),\n",
       " ('un__', 1),\n",
       " ('__di', 1),\n",
       " (\"qu'\", 1),\n",
       " ('lève', 1),\n",
       " ('lheu', 1),\n",
       " ('eur_', 1),\n",
       " ('temp', 1),\n",
       " ('élè', 1),\n",
       " (\"_qu'\", 1),\n",
       " ('r___', 1),\n",
       " ('__s', 1),\n",
       " ('_élè', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Sorting n-grams by frequency (n = 4)¶\n",
    "freq_4grams = {}\n",
    "\n",
    "for ngram in ng_list_4grams:\n",
    "    if ngram not in freq_4grams:\n",
    "        freq_4grams.update({ngram: 1})\n",
    "    else:\n",
    "        ngram_occurrences = freq_4grams[ngram]\n",
    "        freq_4grams.update({ngram: ngram_occurrences + 1})\n",
    "        \n",
    "from operator import itemgetter # The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n",
    "\n",
    "freq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] # We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n",
    "freq_4grams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"le temps est un grand maître dit on le malheur est qu'il tue ses élèves\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Obtaining n-grams for multiple values of n¶\n",
    "from nltk import everygrams\n",
    "\n",
    "s_clean = ' '.join(s_tokenized) # For the code below we need the raw sentence as opposed to the tokens.\n",
    "s_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l',\n",
       " 'e',\n",
       " 't',\n",
       " 'e',\n",
       " 'm',\n",
       " 'p',\n",
       " 's',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'n',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " 'm',\n",
       " 'a',\n",
       " 'î',\n",
       " 't',\n",
       " 'r',\n",
       " 'e',\n",
       " 'd',\n",
       " 'i',\n",
       " 't',\n",
       " 'o',\n",
       " 'n',\n",
       " 'l',\n",
       " 'e',\n",
       " 'm',\n",
       " 'a',\n",
       " 'l',\n",
       " 'h',\n",
       " 'e',\n",
       " 'u',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'q',\n",
       " 'u',\n",
       " \"'\",\n",
       " 'i',\n",
       " 'l',\n",
       " 't',\n",
       " 'u',\n",
       " 'e',\n",
       " 's',\n",
       " 'e',\n",
       " 's',\n",
       " 'é',\n",
       " 'l',\n",
       " 'è',\n",
       " 'v',\n",
       " 'e',\n",
       " 's',\n",
       " 'le',\n",
       " 'e_',\n",
       " '_t',\n",
       " 'te',\n",
       " 'em',\n",
       " 'mp',\n",
       " 'ps',\n",
       " 's_',\n",
       " '_e',\n",
       " 'es',\n",
       " 'st',\n",
       " 't_',\n",
       " '_u',\n",
       " 'un',\n",
       " 'n_',\n",
       " '_g',\n",
       " 'gr',\n",
       " 'ra',\n",
       " 'an',\n",
       " 'nd',\n",
       " 'd_',\n",
       " '_m',\n",
       " 'ma',\n",
       " 'aî',\n",
       " 'ît',\n",
       " 'tr',\n",
       " 're',\n",
       " 'e_',\n",
       " '_d',\n",
       " 'di',\n",
       " 'it',\n",
       " 't_',\n",
       " '_o',\n",
       " 'on',\n",
       " 'n_',\n",
       " '_l',\n",
       " 'le',\n",
       " 'e_',\n",
       " '_m',\n",
       " 'ma',\n",
       " 'al',\n",
       " 'lh',\n",
       " 'he',\n",
       " 'eu',\n",
       " 'ur',\n",
       " 'r_',\n",
       " '_e',\n",
       " 'es',\n",
       " 'st',\n",
       " 't_',\n",
       " '_q',\n",
       " 'qu',\n",
       " \"u'\",\n",
       " \"'i\",\n",
       " 'il',\n",
       " 'l_',\n",
       " '_t',\n",
       " 'tu',\n",
       " 'ue',\n",
       " 'e_',\n",
       " '_s',\n",
       " 'se',\n",
       " 'es',\n",
       " 's_',\n",
       " '_é',\n",
       " 'él',\n",
       " 'lè',\n",
       " 'èv',\n",
       " 've',\n",
       " 'es',\n",
       " 'le_',\n",
       " '_te',\n",
       " 'tem',\n",
       " 'emp',\n",
       " 'mps',\n",
       " 'ps_',\n",
       " '_es',\n",
       " 'est',\n",
       " 'st_',\n",
       " '_un',\n",
       " 'un_',\n",
       " '_gr',\n",
       " 'gra',\n",
       " 'ran',\n",
       " 'and',\n",
       " 'nd_',\n",
       " '_ma',\n",
       " 'maî',\n",
       " 'aît',\n",
       " 'îtr',\n",
       " 'tre',\n",
       " 're_',\n",
       " '_di',\n",
       " 'dit',\n",
       " 'it_',\n",
       " '_on',\n",
       " 'on_',\n",
       " '_le',\n",
       " 'le_',\n",
       " '_ma',\n",
       " 'mal',\n",
       " 'alh',\n",
       " 'lhe',\n",
       " 'heu',\n",
       " 'eur',\n",
       " 'ur_',\n",
       " '_es',\n",
       " 'est',\n",
       " 'st_',\n",
       " '_qu',\n",
       " \"qu'\",\n",
       " \"u'i\",\n",
       " \"'il\",\n",
       " 'il_',\n",
       " '_tu',\n",
       " 'tue',\n",
       " 'ue_',\n",
       " '_se',\n",
       " 'ses',\n",
       " 'es_',\n",
       " '_él',\n",
       " 'élè',\n",
       " 'lèv',\n",
       " 'ève',\n",
       " 'ves',\n",
       " '_tem',\n",
       " 'temp',\n",
       " 'emps',\n",
       " 'mps_',\n",
       " '_est',\n",
       " 'est_',\n",
       " '_un_',\n",
       " '_gra',\n",
       " 'gran',\n",
       " 'rand',\n",
       " 'and_',\n",
       " '_maî',\n",
       " 'maît',\n",
       " 'aîtr',\n",
       " 'ître',\n",
       " 'tre_',\n",
       " '_dit',\n",
       " 'dit_',\n",
       " '_on_',\n",
       " '_le_',\n",
       " '_mal',\n",
       " 'malh',\n",
       " 'alhe',\n",
       " 'lheu',\n",
       " 'heur',\n",
       " 'eur_',\n",
       " '_est',\n",
       " 'est_',\n",
       " \"_qu'\",\n",
       " \"qu'i\",\n",
       " \"u'il\",\n",
       " \"'il_\",\n",
       " '_tue',\n",
       " 'tue_',\n",
       " '_ses',\n",
       " 'ses_',\n",
       " '_élè',\n",
       " 'élèv',\n",
       " 'lève',\n",
       " 'èves']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram_extractor(sent):\n",
    "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n",
    "            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
    "\n",
    "ngram_extractor(s_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('le', 'temps'),\n",
       " ('temps', 'est'),\n",
       " ('est', 'un'),\n",
       " ('un', 'grand'),\n",
       " ('grand', 'maître')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "bi_trade_words_condensed = list(bigrams(s_tokenized))\n",
    "bi_trade_words_condensed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dit', 'on') 1\n",
      "('on', 'le') 1\n",
      "('est', 'un') 1\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "bi_fdist = FreqDist(bi_trade_words_condensed)\n",
    "\n",
    "for word, frequency in bi_fdist.most_common(3):\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bi_fdist.plot(3, cumulative=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-test re-testing re-test\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import (PorterStemmer, LancasterStemmer)\n",
    "from nltk.stem.snowball import SnowballStemmer # This is \"Porter 2\" and is considered the optimal stemmer.\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "print(porter.stem('Re-testing'), lancaster.stem('Re-testing'), snowball.stem('Re-testing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Fun fact: SnowballStemmer can stem several other languages beside English.\n",
    "# To make, for instance, a French stemmer, we can do the following: french_stemmer = SnowballStemmer('french')\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "sentence = \"So, we'll go no more a-roving. So late into the night, Though the heart be still as loving, And the moon be still as bright.\"\n",
    "\n",
    "# This uses the 3-argument version of str.maketrans with arguments (x, y, z) where 'x' and 'y' must be equal-length strings and characters in 'x' are replaced by characters in 'y'. 'z' is a string (string.punctuation here) where each character in the string is mapped to None\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "translator\n",
    "\n",
    "# This is an alternative that creates a dictionary mapping of every character from string.punctuation to None (this will also work but creates a whole dictionary so is slower)\n",
    "#translator = str.maketrans(dict.fromkeys(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Lemmatizing\n",
    "Lemmatization aims to achieve a similar base \"stem\" for a word, but aims to derive the genuine dictionary root word, not just a trunctated version of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brightening box\n"
     ]
    }
   ],
   "source": [
    "# The default lemmatization method with the Python NLTK is the WordNet lemmatizer.\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "print(wnl.lemmatize('brightening'), wnl.lemmatize('boxes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brighten'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# As we saw above, sometimes, if we try to lemmatize a word, it will end up with the same word. This is because the default part of speech is nouns.\n",
    "wnl.lemmatize('brightening', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Chunking and Entity Recognition\n",
    "\n",
    "The goal of chunking is to divide a sentence into chunks. Usually each chunk contains a head and optionally additionally words and modifiers. Examples of chunks include noun groups and verb groups.\n",
    "\n",
    "4.1. Chunking¶¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('le', 'NN'),\n",
       " ('temps', 'NNS'),\n",
       " ('est', 'VBP'),\n",
       " ('un', 'JJ'),\n",
       " ('grand', 'JJ'),\n",
       " ('maître', 'NN'),\n",
       " ('dit', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('le', 'JJ'),\n",
       " ('malheur', 'NN'),\n",
       " ('est', 'JJS'),\n",
       " (\"qu'il\", 'NN'),\n",
       " ('tue', 'NN'),\n",
       " ('ses', 'VBZ'),\n",
       " ('élèves', 'NNS')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "from nltk import pos_tag\n",
    "tags = pos_tag(s_tokenized)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to create a chunker, we need to first define a chunk grammar, consisting of rules that indicate how sentences should be chunked.\n",
    "\n",
    "We can define a simple grammar for a noun phrase (NP) chunker with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN).\n",
    "\n",
    "Note how grammatical structures which are not noun phrases are not chunked, which is totally fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy_widget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n\u001b[0m\u001b[0;32m    731\u001b[0m                             \u001b[1;34m'-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m                             .format(out_path, in_path).split())\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[0;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[1;32m--> 604\u001b[1;33m                                  binary_names, url, verbose))\n\u001b[0m\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \"\"\"\n\u001b[0;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[1;32m--> 598\u001b[1;33m                      url, verbose):\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    567\u001b[0m                         (filename, url))\n\u001b[0;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n==========================================================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [('le', 'NN')]), ('temps', 'NNS'), ('est', 'VBP'), Tree('NP', [('un', 'JJ'), ('grand', 'JJ'), ('maître', 'NN')]), Tree('NP', [('dit', 'NN')]), ('on', 'IN'), Tree('NP', [('le', 'JJ'), ('malheur', 'NN')]), ('est', 'JJS'), Tree('NP', [(\"qu'il\", 'NN')]), Tree('NP', [('tue', 'NN')]), ('ses', 'VBZ'), ('élèves', 'NNS')])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker = RegexpParser(grammar)\n",
    "result = chunker.parse(tags)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy_widget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n\u001b[0m\u001b[0;32m    731\u001b[0m                             \u001b[1;34m'-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m                             .format(out_path, in_path).split())\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[0;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[1;32m--> 604\u001b[1;33m                                  binary_names, url, verbose))\n\u001b[0m\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \"\"\"\n\u001b[0;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[1;32m--> 598\u001b[1;33m                      url, verbose):\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    567\u001b[0m                         (filename, url))\n\u001b[0;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n==========================================================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('le', 'NN'), ('temps', 'NNS'), ('est', 'VBP'), ('un', 'JJ'), ('grand', 'JJ'), ('maître', 'NN'), ('dit', 'NN'), ('on', 'IN'), ('le', 'JJ'), ('malheur', 'NN'), ('est', 'JJS'), (\"qu'il\", 'NN'), ('tue', 'NN'), ('ses', 'VBZ'), ('élèves', 'NNS')])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.chunk import ne_chunk # ne = named entity\n",
    "ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hb20007/hands-on-nltk-tutorial/blob/master/2-3-Language-Identifier-Using-Word-Bigrams.ipynb\n",
    "#Language identifier using word bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

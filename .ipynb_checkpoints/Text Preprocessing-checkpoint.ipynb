{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# After a text is obtained, we start with text normalization. Text normalization includes:\n",
    "converting all letters to lower or upper case\n",
    "converting numbers into words or removing numbers\n",
    "removing punctuations, accent marks and other diacritics\n",
    "removing white spaces\n",
    "expanding abbreviations\n",
    "removing stop words, sparse terms, and particular words\n",
    "text canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to lower case\n",
    "\n",
    "text= \"1. Dataset Preparation:                  The first step is the Dataset Preparation step which includes the process of loading a dataset and performing basic pre-processing. The dataset is then splitted into train and validation sets.\\\n",
    "2. Feature Engineering: The next step is the Feature Engineering in which the raw dataset is transformed into flat features which can be used in a machine learning model. This step also includes the process of creating new features from the existing data.\\\n",
    "3. Model Training: The final step is the Model Building step in which a machine learning model is trained on a labelled dataset.\\\n",
    "4. Improve Performance of TexT \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. dataset preparation:                  the first step is the dataset preparation step which includes the process of loading a dataset and performing basic pre-processing. the dataset is then splitted into train and validation sets.2. feature engineering: the next step is the feature engineering in which the raw dataset is transformed into flat features which can be used in a machine learning model. this step also includes the process of creating new features from the existing data.3. model training: the final step is the model building step in which a machine learning model is trained on a labelled dataset.4. improve performance of text \n"
     ]
    }
   ],
   "source": [
    "text_lower=text.lower()\n",
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". dataset preparation:                  the first step is the dataset preparation step which includes the process of loading a dataset and performing basic pre-processing. the dataset is then splitted into train and validation sets.. feature engineering: the next step is the feature engineering in which the raw dataset is transformed into flat features which can be used in a machine learning model. this step also includes the process of creating new features from the existing data.. model training: the final step is the model building step in which a machine learning model is trained on a labelled dataset.. improve performance of text \n"
     ]
    }
   ],
   "source": [
    "#Remove numbers\n",
    "\n",
    "import re\n",
    "\n",
    "number_removal=re.sub(r'\\d+', '',text_lower)\n",
    "print(number_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove puncuation\n",
    "#The following code removes this set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a  country  we value each_religion yeah jeffy we are proud \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "input_String= \"this is a $ country & we value each_religion? yeah @jeffy {we are proud} \"\n",
    "#remove_puncuation=input_String.translate(str.maketrans(\"\",\"\"),string.punctuation)\n",
    "remove_puncuation = re.sub(r'[^\\w\\s]','',input_String)\n",
    "print(remove_puncuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a  country  we value eachreligion yeah jeffy we are proud \n"
     ]
    }
   ],
   "source": [
    "#print(string.punctuation)\n",
    "input_String= \"this is a $ country & we value each_religion? yeah @jeffy {we are proud} \"\n",
    "table=str.maketrans(\"\",\"\",string.punctuation)    \n",
    "result=input_String.translate(table) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with punctuation \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "input_str =\"This &is [an] example? {of} string. with.? punctuation!!!! \" \n",
    "table=str.maketrans(\"\",\"\",string.punctuation)    \n",
    "result=input_str.translate(table) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dataset preparation                  the first step is the dataset preparation step which includes the process of loading a dataset and performing basic preprocessing the dataset is then splitted into train and validation sets feature engineering the next step is the feature engineering in which the raw dataset is transformed into flat features which can be used in a machine learning model this step also includes the process of creating new features from the existing data model training the final step is the model building step in which a machine learning model is trained on a labelled dataset improve performance of text \n"
     ]
    }
   ],
   "source": [
    "#print(string.punctuation)\n",
    "\n",
    "import string\n",
    "input_str =\"This &is [an] example? {of} string. with.? punctuation!!!! \" \n",
    "table=str.maketrans(\"\",\"\",string.punctuation)    \n",
    "result=number_removal.translate(table) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove whitespaces\n",
    "\n",
    "To remove leading and ending spaces, you can use the strip() function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset preparation                  the first step is the dataset preparation step which includes the process of loading a dataset and performing basic preprocessing the dataset is then splitted into train and validation sets feature engineering the next step is the feature engineering in which the raw dataset is transformed into flat features which can be used in a machine learning model this step also includes the process of creating new features from the existing data model training the final step is the model building step in which a machine learning model is trained on a labelled dataset improve performance of text'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitespace_removal=result.strip()\n",
    "whitespace_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset', 'preparation', 'the', 'first', 'step', 'is', 'the', 'dataset', 'preparation', 'step', 'which', 'includes', 'the', 'process', 'of', 'loading', 'a', 'dataset', 'and', 'performing', 'basic', 'preprocessing', 'the', 'dataset', 'is', 'then', 'splitted', 'into', 'train', 'and', 'validation', 'sets', 'feature', 'engineering', 'the', 'next', 'step', 'is', 'the', 'feature', 'engineering', 'in', 'which', 'the', 'raw', 'dataset', 'is', 'transformed', 'into', 'flat', 'features', 'which', 'can', 'be', 'used', 'in', 'a', 'machine', 'learning', 'model', 'this', 'step', 'also', 'includes', 'the', 'process', 'of', 'creating', 'new', 'features', 'from', 'the', 'existing', 'data', 'model', 'training', 'the', 'final', 'step', 'is', 'the', 'model', 'building', 'step', 'in', 'which', 'a', 'machine', 'learning', 'model', 'is', 'trained', 'on', 'a', 'labelled', 'dataset', 'improve', 'performance', 'of', 'text']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(whitespace_removal)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God is Great!', 'I won a lottery.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "#Output: ['God is Great!', 'I won a lottery ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset', 'preparation', 'first', 'step', 'dataset', 'preparation', 'step', 'includes', 'process', 'loading', 'dataset', 'performing', 'basic', 'preprocessing', 'dataset', 'splitted', 'train', 'validation', 'sets', 'feature', 'engineering', 'next', 'step', 'feature', 'engineering', 'raw', 'dataset', 'transformed', 'flat', 'features', 'used', 'machine', 'learning', 'model', 'step', 'also', 'includes', 'process', 'creating', 'new', 'features', 'existing', 'data', 'model', 'training', 'final', 'step', 'model', 'building', 'step', 'machine', 'learning', 'model', 'trained', 'labelled', 'dataset', 'improve', 'performance', 'text']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))  # AS stopwords is not iterable we will save and remove\n",
    "stopword_removal=[w for w in tokens if not w in stop_words]\n",
    "print(stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "prepar\n",
      "first\n",
      "step\n",
      "dataset\n",
      "prepar\n",
      "step\n",
      "includ\n",
      "process\n",
      "load\n",
      "dataset\n",
      "perform\n",
      "basic\n",
      "preprocess\n",
      "dataset\n",
      "split\n",
      "train\n",
      "valid\n",
      "set\n",
      "featur\n",
      "engin\n",
      "next\n",
      "step\n",
      "featur\n",
      "engin\n",
      "raw\n",
      "dataset\n",
      "transform\n",
      "flat\n",
      "featur\n",
      "use\n",
      "machin\n",
      "learn\n",
      "model\n",
      "step\n",
      "also\n",
      "includ\n",
      "process\n",
      "creat\n",
      "new\n",
      "featur\n",
      "exist\n",
      "data\n",
      "model\n",
      "train\n",
      "final\n",
      "step\n",
      "model\n",
      "build\n",
      "step\n",
      "machin\n",
      "learn\n",
      "model\n",
      "train\n",
      "label\n",
      "dataset\n",
      "improv\n",
      "perform\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "for word in stopword_removal:\n",
    "    print(stemmer.stem(word))\n",
    "    #result=stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "prep\n",
      "first\n",
      "step\n",
      "dataset\n",
      "prep\n",
      "step\n",
      "includ\n",
      "process\n",
      "load\n",
      "dataset\n",
      "perform\n",
      "bas\n",
      "preprocess\n",
      "dataset\n",
      "splitted\n",
      "train\n",
      "valid\n",
      "set\n",
      "feat\n",
      "engin\n",
      "next\n",
      "step\n",
      "feat\n",
      "engin\n",
      "raw\n",
      "dataset\n",
      "transform\n",
      "flat\n",
      "feat\n",
      "us\n",
      "machin\n",
      "learn\n",
      "model\n",
      "step\n",
      "also\n",
      "includ\n",
      "process\n",
      "cre\n",
      "new\n",
      "feat\n",
      "ex\n",
      "dat\n",
      "model\n",
      "train\n",
      "fin\n",
      "step\n",
      "model\n",
      "build\n",
      "step\n",
      "machin\n",
      "learn\n",
      "model\n",
      "train\n",
      "label\n",
      "dataset\n",
      "improv\n",
      "perform\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "for word in stopword_removal:\n",
    "    print(lancaster.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example why lemmatization is better then stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\santhob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "dataset\n",
      "preparation\n",
      "first\n",
      "step\n",
      "dataset\n",
      "preparation\n",
      "step\n",
      "includes\n",
      "process\n",
      "loading\n",
      "dataset\n",
      "performing\n",
      "basic\n",
      "preprocessing\n",
      "dataset\n",
      "splitted\n",
      "train\n",
      "validation\n",
      "set\n",
      "feature\n",
      "engineering\n",
      "next\n",
      "step\n",
      "feature\n",
      "engineering\n",
      "raw\n",
      "dataset\n",
      "transformed\n",
      "flat\n",
      "feature\n",
      "used\n",
      "machine\n",
      "learning\n",
      "model\n",
      "step\n",
      "also\n",
      "includes\n",
      "process\n",
      "creating\n",
      "new\n",
      "feature\n",
      "existing\n",
      "data\n",
      "model\n",
      "training\n",
      "final\n",
      "step\n",
      "model\n",
      "building\n",
      "step\n",
      "machine\n",
      "learning\n",
      "model\n",
      "trained\n",
      "labelled\n",
      "dataset\n",
      "improve\n",
      "performance\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "for word in stopword_removal:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize Single Word\n",
    "print(lemmatizer.lemmatize(\"bats\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging (POS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dataset', 'NN'), ('preparation', 'NN'), ('first', 'RB'), ('step', 'NN'), ('dataset', 'VBN'), ('preparation', 'JJ'), ('step', 'NN'), ('includes', 'VBZ'), ('process', 'NN'), ('loading', 'VBG'), ('dataset', 'NN'), ('performing', 'VBG'), ('basic', 'JJ'), ('preprocessing', 'NN'), ('dataset', 'NN'), ('splitted', 'VBD'), ('train', 'JJ'), ('validation', 'NN'), ('sets', 'NNS'), ('feature', 'VBP'), ('engineering', 'NN'), ('next', 'JJ'), ('step', 'NN'), ('feature', 'NN'), ('engineering', 'NN'), ('raw', 'JJ'), ('dataset', 'NN'), ('transformed', 'VBD'), ('flat', 'JJ'), ('features', 'NNS'), ('used', 'VBN'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'JJ'), ('step', 'NN'), ('also', 'RB'), ('includes', 'VBZ'), ('process', 'NN'), ('creating', 'VBG'), ('new', 'JJ'), ('features', 'NNS'), ('existing', 'VBG'), ('data', 'NNS'), ('model', 'NN'), ('training', 'VBG'), ('final', 'JJ'), ('step', 'NN'), ('model', 'NN'), ('building', 'VBG'), ('step', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('trained', 'VBD'), ('labelled', 'JJ'), ('dataset', 'NN'), ('improve', 'VB'), ('performance', 'NN'), ('text', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(stopword_removal)\n",
    "print(tagged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking (shallow parsing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = ('''\n",
    "    NP: {<DT>?<JJ>*<NN>} # NP\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dataset', 'NN'),\n",
       " ('preparation', 'NN'),\n",
       " ('first', 'RB'),\n",
       " ('step', 'NN'),\n",
       " ('dataset', 'VBN'),\n",
       " ('preparation', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('includes', 'VBZ'),\n",
       " ('process', 'NN'),\n",
       " ('loading', 'VBG'),\n",
       " ('dataset', 'NN'),\n",
       " ('performing', 'VBG'),\n",
       " ('basic', 'JJ'),\n",
       " ('preprocessing', 'NN'),\n",
       " ('dataset', 'NN'),\n",
       " ('splitted', 'VBD'),\n",
       " ('train', 'JJ'),\n",
       " ('validation', 'NN'),\n",
       " ('sets', 'NNS'),\n",
       " ('feature', 'VBP'),\n",
       " ('engineering', 'NN'),\n",
       " ('next', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('feature', 'NN'),\n",
       " ('engineering', 'NN'),\n",
       " ('raw', 'JJ'),\n",
       " ('dataset', 'NN'),\n",
       " ('transformed', 'VBD'),\n",
       " ('flat', 'JJ'),\n",
       " ('features', 'NNS'),\n",
       " ('used', 'VBN'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('model', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('also', 'RB'),\n",
       " ('includes', 'VBZ'),\n",
       " ('process', 'NN'),\n",
       " ('creating', 'VBG'),\n",
       " ('new', 'JJ'),\n",
       " ('features', 'NNS'),\n",
       " ('existing', 'VBG'),\n",
       " ('data', 'NNS'),\n",
       " ('model', 'NN'),\n",
       " ('training', 'VBG'),\n",
       " ('final', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('model', 'NN'),\n",
       " ('building', 'VBG'),\n",
       " ('step', 'NN'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('model', 'NN'),\n",
       " ('trained', 'VBD'),\n",
       " ('labelled', 'JJ'),\n",
       " ('dataset', 'NN'),\n",
       " ('improve', 'VB'),\n",
       " ('performance', 'NN'),\n",
       " ('text', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkParser = nltk.RegexpParser(grammar)\n",
    "#tagged = nltk.pos_tag(nltk.word_tokenize(stopword_removal))\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP dataset/NN)\n",
      "  (NP preparation/NN)\n",
      "  first/RB\n",
      "  (NP step/NN)\n",
      "  dataset/VBN\n",
      "  (NP preparation/JJ step/NN)\n",
      "  includes/VBZ\n",
      "  (NP process/NN)\n",
      "  loading/VBG\n",
      "  (NP dataset/NN)\n",
      "  performing/VBG\n",
      "  (NP basic/JJ preprocessing/NN)\n",
      "  (NP dataset/NN)\n",
      "  splitted/VBD\n",
      "  (NP train/JJ validation/NN)\n",
      "  sets/NNS\n",
      "  feature/VBP\n",
      "  (NP engineering/NN)\n",
      "  (NP next/JJ step/NN)\n",
      "  (NP feature/NN)\n",
      "  (NP engineering/NN)\n",
      "  (NP raw/JJ dataset/NN)\n",
      "  transformed/VBD\n",
      "  flat/JJ\n",
      "  features/NNS\n",
      "  used/VBN\n",
      "  (NP machine/NN)\n",
      "  learning/VBG\n",
      "  (NP model/JJ step/NN)\n",
      "  also/RB\n",
      "  includes/VBZ\n",
      "  (NP process/NN)\n",
      "  creating/VBG\n",
      "  new/JJ\n",
      "  features/NNS\n",
      "  existing/VBG\n",
      "  data/NNS\n",
      "  (NP model/NN)\n",
      "  training/VBG\n",
      "  (NP final/JJ step/NN)\n",
      "  (NP model/NN)\n",
      "  building/VBG\n",
      "  (NP step/NN)\n",
      "  (NP machine/NN)\n",
      "  learning/VBG\n",
      "  (NP model/NN)\n",
      "  trained/VBD\n",
      "  (NP labelled/JJ dataset/NN)\n",
      "  improve/VB\n",
      "  (NP performance/NN)\n",
      "  (NP text/NN))\n",
      "(NP dataset/NN)\n",
      "(NP preparation/NN)\n",
      "(NP step/NN)\n",
      "(NP preparation/JJ step/NN)\n",
      "(NP process/NN)\n",
      "(NP dataset/NN)\n",
      "(NP basic/JJ preprocessing/NN)\n",
      "(NP dataset/NN)\n",
      "(NP train/JJ validation/NN)\n",
      "(NP engineering/NN)\n",
      "(NP next/JJ step/NN)\n",
      "(NP feature/NN)\n",
      "(NP engineering/NN)\n",
      "(NP raw/JJ dataset/NN)\n",
      "(NP machine/NN)\n",
      "(NP model/JJ step/NN)\n",
      "(NP process/NN)\n",
      "(NP model/NN)\n",
      "(NP final/JJ step/NN)\n",
      "(NP model/NN)\n",
      "(NP step/NN)\n",
      "(NP machine/NN)\n",
      "(NP model/NN)\n",
      "(NP labelled/JJ dataset/NN)\n",
      "(NP performance/NN)\n",
      "(NP text/NN)\n"
     ]
    }
   ],
   "source": [
    "tree = chunkParser.parse(tagged)\n",
    "for subtree in tree.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-37c0e87b975b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#input_str = “Bill works for Apple so he went to Boston for a conference.”\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopword_removal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;31m# Standard word tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1239\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[1;32m-> 1241\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \"\"\"\n\u001b[1;32m-> 1291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \"\"\"\n\u001b[1;32m-> 1291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1281\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1282\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \"\"\"\n\u001b[0;32m   1321\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \"\"\"\n\u001b[0;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m     \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "#input_str = “Bill works for Apple so he went to Boston for a conference.”\n",
    "print(ne_chunk(pos_tag(word_tokenize(stopword_removal))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  Apple/NNP\n",
      "  so/IN\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Boston/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  conference/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n",
    "print (ne_chunk(pos_tag(word_tokenize(input_str))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference resolution (anaphora resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collocation extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationship extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data =stopword_Removal\n",
    "def lemmatize_stemming(stopword_removal):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(stopword_removal, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset',\n",
       " 'preparation',\n",
       " 'first',\n",
       " 'step',\n",
       " 'dataset',\n",
       " 'preparation',\n",
       " 'step',\n",
       " 'includes',\n",
       " 'process',\n",
       " 'loading',\n",
       " 'dataset',\n",
       " 'performing',\n",
       " 'basic',\n",
       " 'preprocessing',\n",
       " 'dataset',\n",
       " 'splitted',\n",
       " 'train',\n",
       " 'validation',\n",
       " 'sets',\n",
       " 'feature',\n",
       " 'engineering',\n",
       " 'next',\n",
       " 'step',\n",
       " 'feature',\n",
       " 'engineering',\n",
       " 'raw',\n",
       " 'dataset',\n",
       " 'transformed',\n",
       " 'flat',\n",
       " 'features',\n",
       " 'used',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'step',\n",
       " 'also',\n",
       " 'includes',\n",
       " 'process',\n",
       " 'creating',\n",
       " 'new',\n",
       " 'features',\n",
       " 'existing',\n",
       " 'data',\n",
       " 'model',\n",
       " 'training',\n",
       " 'final',\n",
       " 'step',\n",
       " 'model',\n",
       " 'building',\n",
       " 'step',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'trained',\n",
       " 'labelled',\n",
       " 'dataset',\n",
       " 'improve',\n",
       " 'performance',\n",
       " 'text']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting text to bag of words\n",
    "Prior to topic modelling, we convert the tokenized and lemmatized text to a bag of words — which you can think of as a dictionary where the key is the word and value is the number of times that word occurs in the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-bee533b6a5ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopword_removal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(stopword_removal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_sms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-e2c60d4416c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautocorrect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspell\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenized_sms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_sms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_sms' is not defined"
     ]
    }
   ],
   "source": [
    "from autocorrect import spell\n",
    "tokenized_sms[i] = stemmer.stem(spell(tokenized_sms[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
